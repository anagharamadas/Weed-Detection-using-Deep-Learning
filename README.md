Chemical pesticide spraying over a large area is not only a waste of herbicides and labour, but it also pollutes the environment and compromises food quality. As a result, properly identifying weeds and spraying them are critical tactics for increasing agricultural sustainability. In this work, the models were trained using a dataset containing 17,509 labelled images of weeds native to Australia. Despite the advances of Deep learning models, it still faces challenges like high computational power, overfitting and need of a balanced labelled dataset, therefore transformers used mainly in Natural language processing tasks can be considered as a possible replacement for CNNs. This paper aims to analyse and compare Vision Transformer with CNN models for achieving good results on weed detection and classification from crop images. As the final results of the experiment, the Vision transformer gave the highest accuracy of 96.41% followed by the ResNet-50 model which gave an accuracy of 95.70% compared to 95.04% of the Xception model. The Inception V3 model gave an accuracy of 94.7% and Inception-Resnet V2 gave an accuracy of 94.15% on the dataset images.
Detailed description given in the pdf attached.
